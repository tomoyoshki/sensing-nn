# data paths  
pretrain_index_file: /home/tkimura4/data/datasets/RealWorld-HAR/time_data_partition/train_index.txt
activity_classification:
        num_classes: 8 
        class_names: ["climbingdown", "climbingup", "jumping", "lying", "running", "sitting", "standing", "walking"]
        train_index_file: /home/tkimura4/data/datasets/RealWorld-HAR/time_data_partition/train_index.txt
        val_index_file: /home/tkimura4/data/datasets/RealWorld-HAR/time_data_partition/test_index.txt
        test_index_file: /home/tkimura4/data/datasets/RealWorld-HAR/time_data_partition/test_index.txt

base_experiment_dir: /home/madhav5/sensing-nn/src2/experiments

cross_entropy:
        name: "cross_entropy"

label_smoothing_ce:
        name: "label_smoothing_ce"
        label_smoothing: 0.1

kurtosis:
        name: "kurtosis"
        kurtosis_weight: 0.1
        kurtosis_target: 1.8
        kurtosis_eps: 1e-8

importance_vector_loss:
        name: "importance_vector_loss"
        lambda_variance: 0.1
        variance_metric: "loss" # Not used anymore
        target_avg_bitwidth: 6.0
        budget_coeff: 0.01

loss_name: "cross_entropy"

repetition: 3

batch_size: 128
num_workers: 4
use_balanced_sampling: True

# Normalization
# Options: "standard", "minmax", "maxabs", "none"
normalization:
        method: "minmax"
        minmax_range: [0, 1]


# Segments
num_segments: 9

# Locations
num_locaiton: 5
# location_names: ["head", "chest", "upperarm", "waist", "shin"]
location_names: ["waist"]

# Modality names
num_sensors: 4
modality_names: ["acc", "gyr", "mag", "lig"]

# For sequence-based contrastive learning 
seq_len: 4

quantization:
        enable: True
        bn_type: "float"  # float, switchable, transitional
        # Conv: "QuanConv" # QuanConv (for Joint Quantization, and Single Precision Training), QuanConvImportance (for importance vector quantization)
        importance_vector_comprehensive_validation:
                name: "importance_vector_comprehensive_validation"
                number_of_configs: 8
                bitwidth_options: [4,8]
                bin_tolerance: 0.5
        random_bitwidths:
                bitwidth_options: [8,16,32]
                name: "random_bitwidths"
                number_of_configs: 8
                bin_tolerance: 0.5
        joint_quantization:
                joint_quantization_batch_size: 2
                every_epoch_new_bitwidth_config: True
        vanilla_single_precision_training:
                bitwidth_options: [8]
        any_precision:
                Conv: "QuanConv"
                loss_name: "cross_entropy"
                bitwidth_options: [8,16,32]
                weight_quantization: "lsq"
                activation_quantization: "lsq"
                validation_function: "random_bitwidths"
                training_method: "joint_quantization"
                switchable_clipping: False
                sat_weight_normalization: False
                bn_type: "switchable"
        robustquant:
                Conv: "QuanConv"
                bitwidth_options: [8,16,32]
                weight_quantization: "dorefa"
                activation_quantization: "dorefa"
                validation_function: "random_bitwidths"
                training_method: "joint_quantization"
                switchable_clipping: True
                sat_weight_normalization: False
                loss_name: "kurtosis"
                kurtosis_regularization: True
                kurtosis_target: 1.8
                kurtosis_weight: 0.1
        adabits:
                Conv: "QuanConv"
                loss_name: "cross_entropy" # cross_entropy, label_smoothing_ce 
                bitwidth_options: [4, 8]
                default_bitwidth: 8
                bitwidht_allocation: "random"
                weight_quantization: "dorefa"
                activation_quantization: "pact"
                act_quant: "pact"  # Enable PACT activation quantization after ReLU
                validation_function: "random_bitwidths" # simple_validation, random_bitwidths
                training_method: "joint_quantization" # joint_quantization, vanilla_single_precision_training
                switchable_clipping: True
                sat_weight_normalization: False
        # pact:
        #         Conv: "QuanConv"
        #         loss_name: "cross_entropy"
        #         bitwidth_options: [4, 8]
        #         default_bitwidth: 8
        #         bitwidht_allocation: "random"
        #         weight_quantization: "dorefa"
        #         activation_quantization: "pact"
        #         act_quant: "pact"
        #         validation_function: "random_bitwidths"
        #         training_method: "joint_quantization"
        #         switchable_clipping: True
        #         sat_weight_normalization: False
        # lsq:
        #         Conv: "QuanConv"
        #         loss_name: "cross_entropy"
        #         bitwidth_options: [4, 8]
        #         default_bitwidth: 8
        #         bitwidht_allocation: "random"
        #         weight_quantization: "lsq"
        #         activation_quantization: "lsq"
        #         validation_function: "random_bitwidths"
        #         training_method: "joint_quantization"
        #         switchable_clipping: False  # Not used for LSQ
        #         sat_weight_normalization: False
        # lsqplus:
        #         Conv: "QuanConv"
        #         loss_name: "cross_entropy"
        #         bitwidth_options: [4, 8]
        #         default_bitwidth: 8
        #         bitwidht_allocation: "random"
        #         weight_quantization: "lsqplus"
        #         activation_quantization: "lsqplus"
        #         validation_function: "random_bitwidths"
        #         training_method: "joint_quantization"
        #         switchable_clipping: False  # Not used for LSQPlus
        #         sat_weight_normalization: False
        importance_vector:
                Conv: "QuanConvImportance"
                loss_name: "importance_vector_loss" # cross_entropy, label_smoothing_ce, importance_vector_loss
                bitwidth_options: [2, 4, 8]
                weight_quantization: "dorefa"
                activation_quantization: "dorefa"
                validation_function: "importance_vector_comprehensive_validation"
                training_method: "joint_quantization_with_importance_vector"
                number_of_configs: 2
                switchable_clipping: True
                sat_weight_normalization: False
                
                # Importance vector specific params
                importance_vector:
                        sampling_strategy: "soft_gumbel_softmax" # 'best_bitwidth', 'soft_gumbel_softmax', 'uniform_sampling'
                        
                
                # Temperature scheduler for Gumbel-Softmax
                # Used by: models/Temperature_Scheduler.py
                # Similar to LR scheduler, call temp_scheduler.step() at end of each epoch
                temp_scheduler:
                        # Scheduler type: 'cosine', 'exponential', 'linear', 'step', 'cyclic', 'constant'
                        name: "cosine"
                        temp_start: 1.0     # High temperature = more exploration
                        temp_min: 0.1       # Low temperature = more exploitation
                        # num_epochs is taken from lr_scheduler.train_epochs automatically
                        
                        # Additional params for specific schedulers:
                        # For 'exponential': decay_rate (optional, computed if not provided)
                        # For 'step': milestones: [30, 60, 80], gamma: 0.5
                        # For 'cyclic': cycle_length: 20
                
                # Visualization frequency
                visualize_importance_every: 2  # epochs

# Location modalities
loc_mod_in_freq_channels:
        waist: 
                acc: 6
                gyr: 6
                mag: 6
                lig: 2
loc_mod_in_time_channels:
        waist: 
                acc: 3
                gyr: 3
                mag: 3
                lig: 1
loc_mod_spectrum_len:
        waist: 
                acc: 50
                gyr: 50
                mag: 50
                lig: 50

# DeepSense config
DeepSense:
        dropout_ratio: 0.2
        # single interval + location + modality
        loc_mod_in_conv_stride:
                acc: [1, 1]
                gyr: [1, 1]
                mag: [1, 1]
                lig: [1, 1]    
        loc_mod_conv_lens:
                acc: [[1, 3], [1, 3], [1, 3]]
                gyr: [[1, 3], [1, 3], [1, 3]]
                mag: [[1, 3], [1, 3], [1, 3]]
                lig: [[1, 3], [1, 3], [1, 3]] 
        loc_mod_out_channels: 128
        loc_mod_conv_inter_layers: 4
        # single interval + location
        loc_conv_lens: [[1, 4], [1, 4], [1, 4]]
        loc_out_channels: 256
        loc_conv_inter_layers: 3
        # recurrent layer
        recurrent_dim: 256
        recurrent_layers: 2
        # FC layer
        fc_dim: 256
        pretrained_head: "linear"
        # Optimizer config
        optimizer:
                name: "AdamW"
                start_lr: 0.00005
                warmup_lr: 0.000001
                min_lr: 0.0000001
                clip_grad: 5.0
                weight_decay: 0.05
        # LR scheduler
        lr_scheduler:
                name: "cosine"
                warmup_prefix: True
                warmup_epochs: 0
                train_epochs: 500
                start_epoch: 0
                decay_epochs: 100
                decay_rate: 0.2
        # Data Augmenters:
        random_augmenters:
                time_augmenters: ["permutation", "negation", "time_warp", "horizontal_flip", "mag_warp", "scaling", "jitter", "channel_shuffle"]
                freq_augmenters: ["phase_shift"]
        fixed_augmenters:
                time_augmenters: ["mixup"]
                freq_augmenters: ["phase_shift"]

TransformerV4:
        dropout_ratio: 0.2
        drop_path_rate: 0.1
        attn_drop_rate: 0.2
        # loc mod freq feature
        in_stride:
                acc: 1
                gyr: 1
                mag: 1
                lig: 1
        # single modality
        time_freq_out_channels: 32
        time_freq_head_num: 4
        time_freq_block_num: 
                acc: [2, 2, 2]
                gyr: [2, 2, 2]
                mag: [2, 2, 2]
                lig: [2, 2, 2]
        # modality fusion
        mod_out_channels: 128
        mod_head_num: 4
        mod_block_num: 2
        # location fusion
        loc_out_channels: 128 # loc_out_channels == mod_out_channels
        loc_head_num: 4
        loc_block_num: 2
        # SwinTransformer configs
        window_size: 
                acc: [3, 3]
                gyr: [3, 3]
                mag: [3, 3]
                lig: [3, 3]
        mlp_ratio: 4.
        qkv_bias: True
        APE: False
        patch_norm: True
        patch_size:
                freq:
                        acc: [1, 2]
                        gyr: [1, 2]
                        mag: [1, 2]
                        lig: [1, 2]
        # FC layer
        fc_dim: 256
        pretrained_head: "linear"
        # Optimizer config
        optimizer:
                name: "AdamW"
                start_lr: 0.00005
                warmup_lr: 0.000001
                min_lr: 0.0000001
                clip_grad: 5.0
                weight_decay: 0.05
        # LR scheduler
        lr_scheduler:
                name: "cosine"
                warmup_prefix: True
                warmup_epochs: 0
                train_epochs: 200
                start_epoch: 0
                decay_epochs: 50
                decay_rate: 0.2
        # Data Augmenters:
        random_augmenters:
                time_augmenters: ["permutation", "negation", "time_warp", "horizontal_flip", "mag_warp", "scaling", "jitter", "channel_shuffle"]
                freq_augmenters: ["phase_shift"]
        fixed_augmenters:
                time_augmenters: ["mixup"]
                freq_augmenters: ["phase_shift"]

# ------------------------------------------- Data Augmenter Configs ------------------------------------------
# Mixup config
mixup:
        mixup_alpha: 1.0
        cutmix_alpha: 1.0
        cutmix_minmax: null
        prob: 1.0
        switch_prob: 0.75
        mode: 'random_batch'
        label_smoothing: 0
        num_classes: 8

jitter:
        std_in_percent: 0.2
        prob: 0.5

rotation:
        angles: [90, 180, 270]

permutation:
        prob: 0.5

scaling:
        prob: 0.5
        std: 0.2

time_warp:
        prob: 0.5
        magnitude: 0.2
        order: 6

mag_warp:
        prob: 0.5
        magnitude: 0.05
        order: 4

negation:
        prob: 0.5

channel_shuffle:
        prob: 0.5

freq_mask:
        prob: 0.5 
        mask_ratio: 0.3

time_mask:
        prob: 0.5
        mask_ratio: 0.3

phase_shift:
        prob: 0.5 

horizontal_flip:
        prob: 0.5

MobileNetV2:
        # Architecture settings
        width_mult: 1.0
        dropout_ratio: 0.2
        fc_dim: 256

  # Optimizer config
        optimizer:
                name: "AdamW"
                start_lr: 0.0001
                warmup_lr: 0.000001
                min_lr: 0.000001
                clip_grad: 5.0
                weight_decay: 0.05

  # LR scheduler
        lr_scheduler:
                name: "cosine"
                warmup_prefix: True
                warmup_epochs: 0
                train_epochs: 50
                start_epoch: 0
                decay_epochs: 2
                decay_rate: 0.2

        fixed_augmenters:
                time_augmenters: ["no"]
                freq_augmenters: ["no"]