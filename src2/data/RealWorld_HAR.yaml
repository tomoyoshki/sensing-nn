# data paths  
pretrain_index_file: /home/tkimura4/data/datasets/RealWorld-HAR/time_data_partition/train_index.txt
activity_classification:
        num_classes: 8 
        class_names: ["climbingdown", "climbingup", "jumping", "lying", "running", "sitting", "standing", "walking"]
        train_index_file: /home/tkimura4/data/datasets/RealWorld-HAR/time_data_partition/train_index.txt
        val_index_file: /home/tkimura4/data/datasets/RealWorld-HAR/time_data_partition/test_index.txt
        test_index_file: /home/tkimura4/data/datasets/RealWorld-HAR/time_data_partition/test_index.txt

base_experiment_dir: /home/madhav5/sensing-nn/src2/experiments

cross_entropy:
        name: "cross_entropy"

label_smoothing_ce:
        name: "label_smoothing_ce"
        label_smoothing: 0.1

loss_name: "cross_entropy"

repetition: 3

batch_size: 128
num_workers: 4
use_balanced_sampling: True

# Normalization
# Options: "standard", "minmax", "maxabs", "none"
normalization:
        method: "minmax"
        minmax_range: [0, 1]


# Segments
num_segments: 9

# Locations
num_locaiton: 5
# location_names: ["head", "chest", "upperarm", "waist", "shin"]
location_names: ["waist"]

# Modality names
num_sensors: 4
modality_names: ["acc", "gyr", "mag", "lig"]

# For sequence-based contrastive learning 
seq_len: 4

quantization:
        enable: False
        importance_vector_comprehensive_validation:
                name: "importance_vector_comprehensive_validation"
                number_of_configs: 8
                bitwidth_options: [2, 4, 8]
                bin_tolerance: 0.5
        random_bitwidths:
                bitwidth_options: [2, 4, 8]
                name: "random_bitwidths"
                number_of_configs: 8
                bin_tolerance: 0.5
        joint_quantization:
                joint_quantization_batch_size: 2
                every_epoch_new_bitwidth_config: True
        vanilla_single_precision_training:
                bitwidth_options: [8]
        any_precision:
                joint_quantization_batch_size: 2
                bitwidth_options: [8, 16, 32]
                weight_quantization: "dorefa"
                activation_quantization: "dorefa"
                validation_function: "random_bitwidths"
                training_method: "joint_quantization"
                switchable_clipping: True
                sat_weight_normalization: False
        dorefa:
                Conv: "QuanConv"
                loss_name: "cross_entropy"
                bitwidth_options: [2, 4, 8]
                default_bitwidth: 8
                bitwidht_allocation: "random"
                weight_quantization: "dorefa"
                activation_quantization: "dorefa"
                act_quant: "pact"
                validation_function: "random_bitwidths"
                training_method: "joint_quantization"
                switchable_clipping: True
                sat_weight_normalization: False
        pact:
                Conv: "QuanConv"
                loss_name: "cross_entropy"
                bitwidth_options: [2, 4, 8]
                default_bitwidth: 8
                bitwidht_allocation: "random"
                weight_quantization: "dorefa"
                activation_quantization: "pact"
                act_quant: "pact"
                validation_function: "random_bitwidths"
                training_method: "joint_quantization"
                switchable_clipping: True
                sat_weight_normalization: False
        lsq:
                Conv: "QuanConv"
                loss_name: "cross_entropy"
                bitwidth_options: [2, 4, 8]
                default_bitwidth: 8
                bitwidht_allocation: "random"
                weight_quantization: "lsq"
                activation_quantization: "lsq"
                validation_function: "random_bitwidths"
                training_method: "joint_quantization"
                switchable_clipping: False
                sat_weight_normalization: False
        lsqplus:
                Conv: "QuanConv"
                loss_name: "cross_entropy"
                bitwidth_options: [2, 4, 8]
                default_bitwidth: 8
                bitwidht_allocation: "random"
                weight_quantization: "lsqplus"
                activation_quantization: "lsqplus"
                validation_function: "random_bitwidths"
                training_method: "joint_quantization"
                switchable_clipping: False
                sat_weight_normalization: False

# Location modalities
loc_mod_in_freq_channels:
        waist: 
                acc: 6
                gyr: 6
                mag: 6
                lig: 2
loc_mod_in_time_channels:
        waist: 
                acc: 3
                gyr: 3
                mag: 3
                lig: 1
loc_mod_spectrum_len:
        waist: 
                acc: 50
                gyr: 50
                mag: 50
                lig: 50

# DeepSense config
DeepSense:
        dropout_ratio: 0.2
        # single interval + location + modality
        loc_mod_in_conv_stride:
                acc: [1, 1]
                gyr: [1, 1]
                mag: [1, 1]
                lig: [1, 1]    
        loc_mod_conv_lens:
                acc: [[1, 3], [1, 3], [1, 3]]
                gyr: [[1, 3], [1, 3], [1, 3]]
                mag: [[1, 3], [1, 3], [1, 3]]
                lig: [[1, 3], [1, 3], [1, 3]] 
        loc_mod_out_channels: 128
        loc_mod_conv_inter_layers: 4
        # single interval + location
        loc_conv_lens: [[1, 4], [1, 4], [1, 4]]
        loc_out_channels: 256
        loc_conv_inter_layers: 3
        # recurrent layer
        recurrent_dim: 256
        recurrent_layers: 2
        # FC layer
        fc_dim: 256
        pretrained_head: "linear"
        # Optimizer config
        optimizer:
                name: "AdamW"
                start_lr: 0.00005
                warmup_lr: 0.000001
                min_lr: 0.0000001
                clip_grad: 5.0
                weight_decay: 0.05
        # LR scheduler
        lr_scheduler:
                name: "cosine"
                warmup_prefix: True
                warmup_epochs: 0
                train_epochs: 500
                start_epoch: 0
                decay_epochs: 100
                decay_rate: 0.2
        # Data Augmenters:
        random_augmenters:
                time_augmenters: ["permutation", "negation", "time_warp", "horizontal_flip", "mag_warp", "scaling", "jitter", "channel_shuffle"]
                freq_augmenters: ["phase_shift"]
        fixed_augmenters:
                time_augmenters: ["mixup"]
                freq_augmenters: ["phase_shift"]

TransformerV4:
        dropout_ratio: 0.2
        drop_path_rate: 0.1
        attn_drop_rate: 0.2
        # loc mod freq feature
        in_stride:
                acc: 1
                gyr: 1
                mag: 1
                lig: 1
        # single modality
        time_freq_out_channels: 32
        time_freq_head_num: 4
        time_freq_block_num: 
                acc: [2, 2, 2]
                gyr: [2, 2, 2]
                mag: [2, 2, 2]
                lig: [2, 2, 2]
        # modality fusion
        mod_out_channels: 128
        mod_head_num: 4
        mod_block_num: 2
        # location fusion
        loc_out_channels: 128 # loc_out_channels == mod_out_channels
        loc_head_num: 4
        loc_block_num: 2
        # SwinTransformer configs
        window_size: 
                acc: [3, 3]
                gyr: [3, 3]
                mag: [3, 3]
                lig: [3, 3]
        mlp_ratio: 4.
        qkv_bias: True
        APE: False
        patch_norm: True
        patch_size:
                freq:
                        acc: [1, 2]
                        gyr: [1, 2]
                        mag: [1, 2]
                        lig: [1, 2]
        # FC layer
        fc_dim: 256
        pretrained_head: "linear"
        # Optimizer config
        optimizer:
                name: "AdamW"
                start_lr: 0.00005
                warmup_lr: 0.000001
                min_lr: 0.0000001
                clip_grad: 5.0
                weight_decay: 0.05
        # LR scheduler
        lr_scheduler:
                name: "cosine"
                warmup_prefix: True
                warmup_epochs: 0
                train_epochs: 200
                start_epoch: 0
                decay_epochs: 50
                decay_rate: 0.2
        # Data Augmenters:
        random_augmenters:
                time_augmenters: ["permutation", "negation", "time_warp", "horizontal_flip", "mag_warp", "scaling", "jitter", "channel_shuffle"]
                freq_augmenters: ["phase_shift"]
        fixed_augmenters:
                time_augmenters: ["mixup"]
                freq_augmenters: ["phase_shift"]

# ------------------------------------------- Data Augmenter Configs ------------------------------------------
# Mixup config
mixup:
        mixup_alpha: 1.0
        cutmix_alpha: 1.0
        cutmix_minmax: null
        prob: 1.0
        switch_prob: 0.75
        mode: 'random_batch'
        label_smoothing: 0
        num_classes: 8

jitter:
        std_in_percent: 0.2
        prob: 0.5

rotation:
        angles: [90, 180, 270]

permutation:
        prob: 0.5

scaling:
        prob: 0.5
        std: 0.2

time_warp:
        prob: 0.5
        magnitude: 0.2
        order: 6

mag_warp:
        prob: 0.5
        magnitude: 0.05
        order: 4

negation:
        prob: 0.5

channel_shuffle:
        prob: 0.5

freq_mask:
        prob: 0.5 
        mask_ratio: 0.3

time_mask:
        prob: 0.5
        mask_ratio: 0.3

phase_shift:
        prob: 0.5 

horizontal_flip:
        prob: 0.5

MobileNetV2:
        # Architecture settings
        width_mult: 1.0
        dropout_ratio: 0.2
        fc_dim: 256

  # Optimizer config
        optimizer:
                name: "AdamW"
                start_lr: 0.0001
                warmup_lr: 0.000001
                min_lr: 0.000001
                clip_grad: 5.0
                weight_decay: 0.05

  # LR scheduler
        lr_scheduler:
                name: "cosine"
                warmup_prefix: True
                warmup_epochs: 0
                train_epochs: 50
                start_epoch: 0
                decay_epochs: 2
                decay_rate: 0.2

        fixed_augmenters:
                time_augmenters: ["no"]
                freq_augmenters: ["no"]