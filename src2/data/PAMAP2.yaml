# data paths  
pretrain_index_file: /home/user/madhav/datasets/PAMAP2/time_data_partition/train_index.txt
activity_classification:
        num_classes: 18  # number of classes
        class_names: ["lying", "sitting", "standing", "walking", "running", "cycling", "nordic walking", "watching TV", "computer work", "car driving", "ascending stairs", "descending stairs", "vacuum cleaning", "ironing", "folding laundry", "house cleaning", "playing soccer", "rope jumping"]
        train_index_file: /home/user/madhav/datasets/PAMAP2/time_data_partition/train_index.txt 
        val_index_file: /home/user/madhav/datasets/PAMAP2/time_data_partition/test_index.txt  
        test_index_file: /home/user/madhav/datasets/PAMAP2/time_data_partition/test_index.txt 

# Training hyperparameters
batch_size: 128
num_workers: 4
use_balanced_sampling: False

base_experiment_dir: /home/user/madhav/experiments

cross_entropy:
        name: "cross_entropy"

label_smoothing_ce:
        name: "label_smoothing_ce"
        label_smoothing: 0.1

importance_vector_loss:
        name: "importance_vector_loss"
        lambda_variance: 0.1
        variance_metric: "loss" # Not used anymore
        target_avg_bitwidth: 6.0
        budget_coeff: 0.01

loss_name: "cross_entropy"


normalization:
        # method: "standard"  # Use StandardScaler (mean=0, std=1)
        method: "minmax"  # Use MinMaxScaler (scale to range)
        minmax_range: [0, 1]  # Only used when method is "minmax"
        # method: "maxabs"  # Use MaxAbsScaler (scale to [-1, 1] by max absolute value)
        # method: "none"  # No normalization


repetition: 3


# Segments
num_segments: 9

# Locations 
num_locaiton: 1
location_names: ["hand"]

# Modality names
num_sensors: 3
modality_names: ["Acc", "Gyro", "Mag"]

# Location modalities
loc_mod_in_freq_channels:
        hand:
                Acc: 6
                Gyro: 6
                Mag: 6
loc_mod_in_time_channels:
        hand:
                Acc: 3
                Gyro: 3
                Mag: 3
loc_mod_spectrum_len:
        hand:
                Acc: 40
                Gyro: 40
                Mag: 40

# For sequence-based contrastive learning 
seq_len: 4

# Add your new model here
NEWMODEL:
        optimizer:
                name: "AdamW"
                start_lr: 0.001
                warmup_lr: 0.000001
                min_lr: 0.0000001
                clip_grad: 5.0
                weight_decay: 0.05
        # LR scheduler
        lr_scheduler:
                name: "step"
                warmup_prefix: True
                warmup_epochs: 0
                train_epochs: 500
                start_epoch: 0
                decay_epochs: 100
                decay_rate: 0.2
        fixed_augmenters:
                time_augmenters: ["no"]
                freq_augmenters: ["no"]
        

quantization:
        enable: True

        # Conv: "QuanConv" # QuanConv (for Joint Quantization, and Single Precision Training), QuanConvImportance (for importance vector quantization)
        importance_vector_comprehensive_validation:
                name: "importance_vector_comprehensive_validation"
                number_of_configs: 8
                bitwidth_options: [4,8]
                bin_tolerance: 0.5
        random_bitwidths:
                bitwidth_options: [2,4,8]
                name: "random_bitwidths"
                number_of_configs: 8
                bin_tolerance: 0.5
        joint_quantization:
                joint_quantization_batch_size: 2
                every_epoch_new_bitwidth_config: True
        vanilla_single_precision_training:
                bitwidth_options: [8]
        any_precision:
                Conv: "QuanConv"
                loss_name: "cross_entropy"
                bitwidth_options: [2,4,8]
                weight_quantization: "dorefa"
                activation_quantization: "pact"
                validation_function: "random_bitwidths"
                training_method: "joint_quantization"
                switchable_clipping: False
                sat_weight_normalization: False
                bn_type: "switchable"
        robustquant:
                Conv: "QuanConv"
                bitwidth_options: [2,4,8]
                weight_quantization: "lsq"
                activation_quantization: "lsq"
                validation_function: "random_bitwidths"
                training_method: "joint_quantization"
                switchable_clipping: True
                sat_weight_normalization: False
                loss_name: "kurtosis"
                kurtosis_regularization: True
                kurtosis_target: 1.8
                kurtosis_weight: 0.1
        adabits:
                Conv: "QuanConv"
                loss_name: "cross_entropy" # cross_entropy, label_smoothing_ce 
                bitwidth_options: [2, 4, 8]
                default_bitwidth: 8
                bitwidht_allocation: "random"
                weight_quantization: "dorefa"
                activation_quantization: "pact"
                validation_function: "random_bitwidths" # simple_validation, random_bitwidths
                training_method: "joint_quantization" # joint_quantization, vanilla_single_precision_training
                switchable_clipping: True
                sat_weight_normalization: False
        bitmixer:
                Conv: "QuanConv"
                loss_name: "cross_entropy"
                bitwidth_options: [2, 4, 8]
                weight_quantization: "lsq"
                activation_quantization: "lsq"
                validation_function: "random_bitwidths"
                training_method: "bitmixer"
                switchable_clipping: False
                sat_weight_normalization: False
                bn_type: "transitional"
                bitmixer:
                        # Stage schedule (epochs). Stage III runs for the remaining epochs if omitted.
                        stage1_epochs: 5
                        stage2_epochs: 5
                        # Stage III sigma schedule (global-vs-layerwise mixing probability)
                        sigma_start: 1.0
                        sigma_end: 0.25
        # pact:
        #         Conv: "QuanConv"
        #         loss_name: "cross_entropy"
        #         bitwidth_options: [4, 8]
        #         default_bitwidth: 8
        #         bitwidht_allocation: "random"
        #         weight_quantization: "dorefa"
        #         activation_quantization: "pact"
        #         act_quant: "pact"
        #         validation_function: "random_bitwidths"
        #         training_method: "joint_quantization"
        #         switchable_clipping: True
        #         sat_weight_normalization: False
        # lsq:
        #         Conv: "QuanConv"
        #         loss_name: "cross_entropy"
        #         bitwidth_options: [4, 8]
        #         default_bitwidth: 8
        #         bitwidht_allocation: "random"
        #         weight_quantization: "lsq"
        #         activation_quantization: "lsq"
        #         validation_function: "random_bitwidths"
        #         training_method: "joint_quantization"
        #         switchable_clipping: False  # Not used for LSQ
        #         sat_weight_normalization: False
        # lsqplus:
        #         Conv: "QuanConv"
        #         loss_name: "cross_entropy"
        #         bitwidth_options: [4, 8]
        #         default_bitwidth: 8
        #         bitwidht_allocation: "random"
        #         weight_quantization: "lsqplus"
        #         activation_quantization: "lsqplus"
        #         validation_function: "random_bitwidths"
        #         training_method: "joint_quantization"
        #         switchable_clipping: False  # Not used for LSQPlus
        #         sat_weight_normalization: False
        importance_vector:
                Conv: "QuanConvImportance"
                loss_name: "importance_vector_loss" # cross_entropy, label_smoothing_ce, importance_vector_loss
                bitwidth_options: [2, 4, 8]
                weight_quantization: "dorefa"
                activation_quantization: "dorefa"
                validation_function: "importance_vector_comprehensive_validation"
                training_method: "joint_quantization_with_importance_vector"
                number_of_configs: 2
                switchable_clipping: True
                sat_weight_normalization: False
                
                # Importance vector specific params
                importance_vector:
                        sampling_strategy: "soft_gumbel_softmax" # 'best_bitwidth', 'soft_gumbel_softmax', 'uniform_sampling'
                        
                
                # Temperature scheduler for Gumbel-Softmax
                # Used by: models/Temperature_Scheduler.py
                # Similar to LR scheduler, call temp_scheduler.step() at end of each epoch
                temp_scheduler:
                        # Scheduler type: 'cosine', 'exponential', 'linear', 'step', 'cyclic', 'constant'
                        name: "cosine"
                        temp_start: 1.0     # High temperature = more exploration
                        temp_min: 0.1       # Low temperature = more exploitation
                        # num_epochs is taken from lr_scheduler.train_epochs automatically
                        
                        # Additional params for specific schedulers:
                        # For 'exponential': decay_rate (optional, computed if not provided)
                        # For 'step': milestones: [30, 60, 80], gamma: 0.5
                        # For 'cyclic': cycle_length: 20
                
                # Visualization frequency
                visualize_importance_every: 2  # epochs


                
ResNet:
        # Architecture settings
        model_name: "resnet18"  # Options: resnet18, resnet34, resnet50, resnet101, resnet152
        block_type: "basic"  # "basic" for ResNet18/34, "bottleneck" for ResNet50+
        layers: [2, 2, 2, 2]  # ResNet18
        # layers: [3, 4, 6, 3]  # ResNet34
        # layers: [3, 4, 6, 3]  # ResNet50 (with block_type: "bottleneck")
        dropout_ratio: 0.2
        fc_dim: 256

  # Optimizer config
        optimizer:
                name: "AdamW"
                start_lr: 0.0001
                warmup_lr: 0.000001
                min_lr: 0.000001
                clip_grad: 5.0
                weight_decay: 0.05

  # LR scheduler
        lr_scheduler:
                name: "cosine"
                warmup_prefix: True
                warmup_epochs: 0
                train_epochs: 50
                start_epoch: 0
                decay_epochs: 2
                decay_rate: 0.2

        fixed_augmenters:
                time_augmenters: ["no"]
                freq_augmenters: ["no"]

# DeepSense config
DeepSense:
        dropout_ratio: 0.2 
        # single interval + location + modality
        loc_mod_in_conv_stride:
                Acc: [1, 1]
                Gyro: [1, 1]
                Mag: [1, 1]
        loc_mod_conv_lens: 
                Acc: [[1, 5], [1, 5], [1, 5]]
                Gyro: [[1, 5], [1, 5], [1, 5]]
                Mag: [[1, 5], [1, 5], [1, 5]]
        loc_mod_out_channels: 64
        loc_mod_conv_inter_layers: 2
        # single interval + location
        loc_conv_lens: [[1, 4], [1, 4], [1, 4]]
        loc_out_channels: 64
        loc_conv_inter_layers: 2
        # recurrent layer
        recurrent_dim: 64
        recurrent_layers: 2
        # FC layer
        fc_dim: 128
        pretrained_head: "linear"
        # Optimizer config
        optimizer:
                name: "AdamW"
                start_lr: 0.001
                warmup_lr: 0.000001
                min_lr: 0.0000001
                clip_grad: 5.0
                weight_decay: 0.05
        # LR scheduler
        lr_scheduler:
                name: "step"
                warmup_prefix: True
                warmup_epochs: 0
                train_epochs: 500
                start_epoch: 0
                decay_epochs: 100
                decay_rate: 0.2
        # Augmenters
        random_augmenters:
                time_augmenters: ["no"]
                freq_augmenters: ["phase_shift"]
        fixed_augmenters:
                time_augmenters: ["mixup"]
                freq_augmenters: ["no"]
        
TransformerV4:
        dropout_ratio: 0.2
        drop_path_rate: 0.1
        attn_drop_rate: 0.2
        # loc mod freq feature
        in_stride:
                Acc: 1
                Gyro: 1
                Mag: 1
        # single modality
        time_freq_out_channels: 32
        time_freq_head_num: 4
        time_freq_block_num: 
                Acc: [2, 2, 2]
                Gyro: [2, 2, 2]
                Mag: [2, 2, 2]
        # modality fusion
        mod_out_channels: 128
        mod_head_num: 4
        mod_block_num: 2
        # location fusion
        loc_out_channels: 128 # loc_out_channels == mod_out_channels
        loc_head_num: 4
        loc_block_num: 2
        # SwinTransformer configs
        window_size: 
                Acc: [3, 5] 
                Gyro: [3, 5] 
                Mag: [3, 5]
        mlp_ratio: 2.
        qkv_bias: True
        APE: False
        patch_norm: True
        patch_size:
                freq:
                        Acc: [1, 2] 
                        Gyro: [1, 2] 
                        Mag: [1, 2]
        # FC layer
        fc_dim: 128
        pretrained_head: "linear"
        # Optimizer config
        optimizer:
                name: "AdamW"
                start_lr: 0.0001
                warmup_lr: 0.000001
                min_lr: 0.0000001
                clip_grad: 5.0
                weight_decay: 0.05
        # LR scheduler
        lr_scheduler:
                name: "cosine"
                warmup_prefix: True
                warmup_epochs: 0
                train_epochs: 500
                start_epoch: 0
                decay_epochs: 100
                decay_rate: 0.2
        # Data Augmenters:
        random_augmenters:
                time_augmenters: ["no"]
                freq_augmenters: ["no"]
        fixed_augmenters:
                time_augmenters: ["mixup"]
                freq_augmenters: ["phase_shift"]

# ------------------------------------------- Data Augmenter Configs ------------------------------------------
# Mixup config
mixup:
        mixup_alpha: 1.0
        cutmix_alpha: 1.0
        cutmix_minmax: null
        prob: 1.0
        switch_prob: 0.75
        mode: 'random_batch'
        label_smoothing: 0
        num_classes: 7

jitter:
        std_in_percent: 0.2
        prob: 0.5

rotation:
        angles: [90, 180, 270]

permutation:
        prob: 0.5

scaling:
        prob: 0.5
        std: 0.2

time_warp:
        prob: 0.5
        magnitude: 0.2
        order: 6

mag_warp:
        prob: 0.5
        magnitude: 0.05
        order: 4

negation:
        prob: 0.5

channel_shuffle:
        prob: 0.5

freq_mask:
        prob: 0.5 
        mask_ratio: 0.3

time_mask:
        prob: 0.5
        mask_ratio: 0.3

phase_shift:
        prob: 0.5 

horizontal_flip:
        prob: 0.5

MobileNetV2:
        # Architecture settings
        width_mult: 1.0
        dropout_ratio: 0.2
        fc_dim: 256

  # Optimizer config
        optimizer:
                name: "AdamW"
                start_lr: 0.0001
                warmup_lr: 0.000001
                min_lr: 0.000001
                clip_grad: 5.0
                weight_decay: 0.05

  # LR scheduler
        lr_scheduler:
                name: "cosine"
                warmup_prefix: True
                warmup_epochs: 0
                train_epochs: 50
                start_epoch: 0
                decay_epochs: 2
                decay_rate: 0.2

        fixed_augmenters:
                time_augmenters: ["no"]
                freq_augmenters: ["no"]
